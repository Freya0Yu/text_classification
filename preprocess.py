# -*- coding: utf-8 -*-
import logging
import os.path
import sys
import MySQLdb
import jieba
from gensim import corpora,models,similarities
import numpy as np
from tgrocery import Grocery
from tgrocery.converter import GroceryClassMapping
from tgrocery.learner.learner import *
from tgrocery.classifier import GroceryTextModel
from tgrocery.converter import GroceryTextConverter
from tgrocery.base import GroceryTestResult

def fetch_douban_event(is_train_data):
    data_set=[]
    try:
        if is_train_data:
            conn=MySQLdb.connect(host='localhost',user='root',passwd='qwer1234',db='train',port=3306,charset='utf8')
            cur=conn.cursor()
            cur.execute('select distinct theme ,content from train_douban')
            results = cur.fetchall()
            cur.close()
            conn.close()
            for row in results:
                data_set.append((row[0].encode("utf-8"),row[1].encode("utf-8")))
            return data_set
        else:
            conn=MySQLdb.connect(host='localhost',user='root',passwd='qwer1234',db='train2',port=3306,charset='utf8')
            cur=conn.cursor()
            cur.execute('select distinct theme ,content from train_douban2')
            results = cur.fetchall()
            cur.close()
            conn.close()
            for row in results:
                data_set.append((row[0].encode("utf-8"),row[1].encode("utf-8")))
            return data_set
        
    except MySQLdb.Error,e:
        print "Mysql Error %d: %s" % (e.args[0], e.args[1])
        return []


def get_stopWords(stopWords_fn):
    stopWords_set=[]
    with open(stopWords_fn, 'rb') as f:
        for line in f:
            stopWords_set.append(line.strip('\r\n').decode('utf-8'))
        
    return stopWords_set

def sentence2words(sentence, stopWords=False):
    """ 
    split a sentence into words based on jieba
    """

    # seg_words is a generator
    seg_words = jieba.cut(sentence)
    if stopWords:
        stopWords_fn = 'stop_words_cn.txt'
        stopWords_set = get_stopWords(stopWords_fn)
        #for word in seg_words:
            #if word=='博洛尼亚':
                #print "hhahahhahhahahahhahahaha"
        words = [word for word in seg_words if word not in stopWords_set and word != ' ']
    else:
        words = [word for word in seg_words]
    return words


def word2vec_model():
    sentences=[]
    texts=fetch_douban_event(True)
    for text in texts:
        sentences.append(sentence2words(text[1].strip(), True)) 


    num_features = 100
    min_word_count = 5
    num_workers = 48
    context = 20
    epoch = 20
    sample = 1e-5

    model = models.Word2Vec(
        sentences,
        size=num_features,
        min_count=min_word_count,
        workers=num_workers,
        sample=sample,
        window=context,
        iter=epoch,
    )
    model.save('word2vec_model_CA')
    return model




def preperation(output):
    tfidf_array=get_tfidf(True)
    sentences=[]
    texts=fetch_douban_event(True)
    model=word2vec_model()

    train_set={}
    vecs=[]
    class_map=GroceryClassMapping()
    with open(output, 'w') as w:
        for doc_id,text in enumerate(texts):
            text_count=0
            d_vec=np.zeros(100)
            for word_id, word in enumerate(sentence2words(text[1].strip(),True)):
                if len(model[word])==100: 
                    #weight=tfidf_array[doc_id][word_id][1]
                    weight=1
                    d_vec=np.add(d_vec,weight * model[word])
                    text_count+=1 
            d_vec=d_vec/text_count
            w.write('%s %s\n' % (class_map.to_idx(text[0]), ''.join(' {0}:{1}'.format(index+1, d_vec[index]) for index in range(0,100))))
    return model,class_map
   
def train_model(train_svm_file,param,index):
    svm_model = train(train_svm_file, param, '-s 4')
    filename='svmlearner%d'%index
    svm_model.save(filename, False)
    text_converter = GroceryTextConverter()
    model = GroceryTextModel(text_converter,svm_model)
    return model 


# 预测
def test(w2v_model,gt_model,class_map):
    tfidf_array=get_tfidf(False)
    test_src = fetch_douban_event(False)
    #print gt_model.svm_model
    ture_ys=[]
    predicted_ys=[]
    for doc_id,text in enumerate(test_src):
        feature_vec=[]
        words=sentence2words(text[1].strip(),True)
        doc_vec=np.zeros(100)
        text_count=0
        for word_id,word in enumerate(words):  
            if len(w2v_model[word])==100:
                #weight=tfidf_array[doc_id][word_id][1]
                weight=1
                doc_vec=np.add(doc_vec,weight*w2v_model[word]) 
                text_count+=1
        test_dic={}
        for index in range(0,len(doc_vec)):
            test_dic[index+1]=doc_vec[index]/text_count
        result=gt_model.predict_text_w2v(test_dic,class_map)
        ture_ys.append(class_map.to_idx(text[0]))
        predicted_ys.append(class_map.to_idx(result.predicted_y))
        #print "原有类别"+text[0]+"判定类别"+result.predicted_y

    return ture_ys,predicted_ys

# 数组每一项为[(3, 0.010216301580426908), (4, 0.010282063041339964), (6, 0.007903518554327965), (11, 0.009465677500703757), (36, 0.0065247150246844285), (52, 0.0034842008003703106), (69, 0.023628904970306334), (81, 0.025497852695983737), (83, 0.011667096891052496), (94, 0.03511303722028723), (97, 0.01284173485095862), (110, 0.009391065853006504), (117, 0.011802480750048286), (126, 0.003298615665671499), (127, 0.003298615665671499), (128, 0.06134912197946011), (129, 0.07166188381453067), (130, 0.033198633674478575), (131, 0.020786775836881013), (132, 0.047774589209687114), (133, 0.10123343668622486), (134, 0.057899785362518004), (135, 0.10123343668622486), (136, 0.05342371696933187), (137, 0.025595889999289603), (138, 0.13258178989751618), (139, 0.1518501550293373), (140, 0.11439516624510729), (141, 0.11943647302421778), (142, 0.11439516624510729), (143, 0.05800079455709705), (144, 0.07166188381453067), (145, 0.0461357392017276), (146, 0.047774589209687114), (147, 0.10049087263229786), (148, 0.024171350884397907), (149, 0.0049972576858314235), (150, 0.031182110962834384), (151, 0.011232931011978515), (152, 0.01521948541036385), (153, 0.007348543340123029), (154, 0.033198633674478575), (155, 0.04291593736461761), (156, 0.024672246274202607), (157, 0.012055357777128899), (158, 0.040073808231192276), (159, 0.02191719345213535), (160, 0.047774589209687114), (161, 0.013442921927043714), (162, 0.007027111701588019), (163, 0.05061671834311243), (164, 0.05061671834311243), (165, 0.0309664153929032), (166, 0.032373027252697445), (167, 0.02240080263608374), (168, 0.014373937146050467), (169, 0.04575806649804292), (170, 0.03483616933726773), (171, 0.047774589209687114), (172, 0.05061671834311243), (173, 0.016744415391727045), (174, 0.011574254527251493), (175, 0.01298726712878847), (176, 0.03697675817129228), (177, 0.04575806649804292), (178, 0.02215482817275372), (179, 0.025595889999289603), (180, 0.01081321974473433), (181, 0.08803789096530693), (182, 0.030356504541053257), (183, 0.00872517036222576), (184, 0.11330559538444104), (185, 0.02805383789842599), (186, 0.05061671834311243), (187, 0.025706596346307328), (188, 0.040899414652973407), (189, 0.01667572273257755), (190, 0.009810146808010999), (191, 0.03521515638612277), (192, 0.06134912197946011), (193, 0.016772397676082768), (194, 0.03533209367238675), (195, 0.03866719637139803), (196, 0.06863709974706438), (197, 0.03163449714227438), (198, 0.031182110962834384), (199, 0.047774589209687114), (200, 0.07592507751466865), (201, 0.07592507751466865), (202, 0.05061671834311243), (203, 0.030757159467818398), (204, 0.023067813087099237), (205, 0.039335278120769215), (206, 0.03380854452632852), (207, 0.05061671834311243), (208, 0.032373027252697445), (209, 0.05061671834311243), (210, 0.05061671834311243), (211, 0.03866719637139803), (212, 0.03697675817129228), (213, 0.035615811312887914), (214, 0.01879313459129895), (215, 0.032912489743495504), (216, 0.023171744701546097), (217, 0.03296377671160163), (218, 0.031182110962834384), (219, 0.03163449714227438), (220, 0.012683498945944711), (221, 0.047774589209687114), (222, 0.01995719728271354), (223, 0.03697675817129228), (224, 0.012069677604218012), (225, 0.03489845162161255), (226, 0.01147890547530854), (227, 0.032637570078997716), (228, 0.03349695754409929), (229, 0.029795440945572387), (230, 0.047774589209687114), (231, 0.03866719637139803), (232, 0.033198633674478575), (233, 0.028339981829409062), (234, 0.032637570078997716), (235, 0.02093752472053494), (236, 0.05061671834311243), (237, 0.020988503511943184), (238, 0.03413462903786696), (239, 0.03055396979401857), (240, 0.05061671834311243), (241, 0.03866719637139803), (242, 0.025305798136395753), (243, 0.02073723815422009), (244, 0.01998884238006808), (245, 0.013513018040404766), (246, 0.02135612985665449), (247, 0.031404655143884186), (248, 0.020988503511943184), (249, 0.05061671834311243), (250, 0.05061671834311243), (251, 0.04575806649804292), (252, 0.05061671834311243), (253, 0.0360407628079039), (254, 0.018997609907177554), (255, 0.039335278120769215), (256, 0.035615811312887914), (257, 0.047774589209687114), (258, 0.026893655682514693), (259, 0.05061671834311243), (260, 0.025595889999289603), (261, 0.029617974430630193), (262, 0.03413462903786696), (263, 0.047774589209687114), (264, 0.014976610461973688), (265, 0.04575806649804292), (266, 0.032373027252697445), (267, 0.00955522588153632), (268, 0.029617974430630193), (269, 0.04575806649804292), (270, 0.04419392996583873), (271, 0.03163449714227438), (272, 0.040073808231192276), (273, 0.04575806649804292), (274, 0.03187213186289274), (275, 0.024672246274202607), (276, 0.02665998228040938), (277, 0.05061671834311243), (278, 0.020399943736370718), (279, 0.021302416544607666), (280, 0.00751121959087775), (281, 0.022655723562558423), (282, 0.01985699737227913), (283, 0.047774589209687114), (284, 0.04575806649804292), (285, 0.032118106326222765), (286, 0.040073808231192276), (287, 0.05061671834311243), (288, 0.05061671834311243), (289, 0.019642621247607798), (290, 0.03866719637139803), (291, 0.014373937146050467), (292, 0.013186133031757906), (293, 0.03749622192406723), (294, 0.032373027252697445), (295, 0.04183541001636179), (296, 0.03380854452632852), (297, 0.05061671834311243), (298, 0.05061671834311243), (299, 0.020353056919931157), (300, 0.04575806649804292), (301, 0.039335278120769215), (302, 0.04419392996583873), (303, 0.015108043429932067), (304, 0.01349542818695273), (305, 0.033415104770555065), (306, 0.047774589209687114), (307, 0.0360407628079039), (308, 0.03805728551954809), (309, 0.01947571924547518), (310, 0.040073808231192276), (311, 0.05061671834311243), (312, 0.033198633674478575), (313, 0.028949892681259002), (314, 0.05061671834311243), (315, 0.03749622192406723), (316, 0.040073808231192276), (317, 0.04419392996583873), (318, 0.05061671834311243), (319, 0.05061671834311243), (320, 0.05061671834311243), (321, 0.05061671834311243), (322, 0.016363958680863835), (323, 0.04291593736461761), (324, 0.05342371696933187), (325, 0.05061671834311243), (326, 0.05061671834311243), (327, 0.06863709974706438), (328, 0.2191719345213535), (329, 0.05061671834311243), (330, 0.1327945346979143), (331, 0.17970935202376345), (332, 0.039335278120769215), (333, 0.05061671834311243), (334, 0.10142563357898554), (335, 0.047774589209687114), (336, 0.040073808231192276), (337, 0.05852172870047872), (338, 0.07592507751466865), (339, 0.03697675817129228), (340, 0.07867055624153843), (341, 0.05624433288610084), (342, 0.04979795051171786), (343, 0.08374239386024822), (344, 0.039161645321750524), (345, 0.030164449981465273), (346, 0.0916619093820557), (347, 0.03483616933726773), (348, 0.06527514015799543), (349, 0.05061671834311243), (350, 0.15301315788229883), (351, 0.1126355215199265), (352, 0.05061671834311243), (353, 0.1029534582027476), (354, 0.04575806649804292), (355, 0.025595889999289603), (356, 0.09561639558867824), (357, 0.03521515638612277), (358, 0.07592507751466865), (359, 0.040899414652973407), (360, 0.06629089494875809), (361, 0.07851163785971046), (362, 0.05061671834311243), (363, 0.047774589209687114), (364, 0.047774589209687114), (365, 0.05061671834311243), (366, 0.03749622192406723), (367, 0.0360407628079039), (368, 0.040899414652973407), (369, 0.04575806649804292), (370, 0.04575806649804292), (371, 0.06437390604692642), (372, 0.05624433288610084), (373, 0.09514321379887022), (374, 0.04419392996583873), (375, 0.04183541001636179), (376, 0.05061671834311243), (377, 0.05473972348101584), (378, 0.06437390604692642), (379, 0.04183541001636179), (380, 0.047774589209687114), (381, 0.04291593736461761), (382, 0.04575806649804292), (383, 0.0360407628079039), (384, 0.04318855201327359), (385, 0.04291593736461761), (386, 0.047774589209687114), (387, 0.06863709974706438), (388, 0.029977517492198208), (389, 0.047774589209687114), (390, 0.06437390604692642), (391, 0.05061671834311243), (392, 0.05061671834311243), (393, 0.05061671834311243), (394, 0.05061671834311243), (395, 0.028339981829409062), (396, 0.08838785993167746), (397, 0.04575806649804292), (398, 0.029795440945572387), (399, 0.04575806649804292), (400, 0.03697675817129228), (401, 0.05061671834311243), (402, 0.033198633674478575), (403, 0.04575806649804292), (404, 0.032118106326222765), (405, 0.04291593736461761), (406, 0.039335278120769215), (407, 0.05061671834311243), (408, 0.05061671834311243), (409, 0.047774589209687114), (410, 0.047774589209687114), (411, 0.05061671834311243), (412, 0.057085928279322136), (413, 0.035615811312887914), (414, 0.04419392996583873), (415, 0.040073808231192276), (416, 0.047774589209687114), (417, 0.04183541001636179), (418, 0.04419392996583873), (419, 0.05061671834311243), (420, 0.04575806649804292), (421, 0.03521515638612277), (422, 0.04419392996583873), (423, 0.04183541001636179), (424, 0.047774589209687114), (425, 0.03521515638612277), (426, 0.040073808231192276), (427, 0.03805728551954809), (428, 0.05061671834311243), (429, 0.05342371696933187), (430, 0.05061671834311243), (431, 0.039335278120769215), (432, 0.05061671834311243), (433, 0.04553475681157988), (434, 0.04183541001636179), (435, 0.04524667497219791), (436, 0.040073808231192276), (437, 0.05061671834311243), (438, 0.05061671834311243), (439, 0.040899414652973407), (440, 0.05061671834311243), (441, 0.040899414652973407), (442, 0.032373027252697445), (443, 0.03749622192406723), (444, 0.04419392996583873), (445, 0.05061671834311243), (446, 0.04419392996583873), (447, 0.04419392996583873), (448, 0.05061671834311243), (449, 0.023336797999472123), (450, 0.047774589209687114), (451, 0.0344766262756997), (452, 0.05061671834311243), (453, 0.05061671834311243), (454, 0.05061671834311243), (455, 0.039335278120769215), (456, 0.047774589209687114), (457, 0.018442474052364786), (458, 0.04419392996583873), (459, 0.05061671834311243), (460, 0.04575806649804292), (461, 0.047774589209687114), (462, 0.05061671834311243), (463, 0.040073808231192276), (464, 0.09010190701975976), (465, 0.05061671834311243), (466, 0.047774589209687114), (467, 0.04575806649804292), (468, 0.05061671834311243), (469, 0.047774589209687114), (470, 0.05061671834311243), (471, 0.05061671834311243), (472, 0.03697675817129228), (473, 0.047774589209687114), (474, 0.05061671834311243), (475, 0.04183541001636179), (476, 0.04183541001636179), (477, 0.05061671834311243), (478, 0.07592507751466865), (479, 0.047774589209687114), (480, 0.05061671834311243), (481, 0.0277789182339282), (482, 0.040073808231192276), (483, 0.06863709974706438), (484, 0.04575806649804292), (485, 0.04575806649804292), (486, 0.040073808231192276), (487, 0.039335278120769215), (488, 0.04291593736461761), (489, 0.05061671834311243), (490, 0.05061671834311243), (491, 0.04183541001636179), (492, 0.028339981829409062), (493, 0.05061671834311243), (494, 0.05061671834311243), (495, 0.027013480017823232), (496, 0.03055396979401857), (497, 0.05061671834311243), (498, 0.039335278120769215), (499, 0.03749622192406723), (500, 0.05061671834311243), (501, 0.05061671834311243), (502, 0.04291593736461761), (503, 0.05061671834311243), (504, 0.047774589209687114), (505, 0.04419392996583873), (506, 0.05061671834311243), (507, 0.05061671834311243), (508, 0.05061671834311243), (509, 0.04291593736461761), (510, 0.040899414652973407), (511, 0.047774589209687114), (512, 0.05061671834311243), (513, 0.05061671834311243)]
def normalization(tfidf_array):
    maxNum=0
    minNum=0
    for document in tfidf_array:
        for tfidf in document:
            
            if tfidf[1]>maxNum:
                maxNum=tfidf[1]
                print maxNum
            if tfidf[1]<minNum:
                minNum=tfidf[1]

    for document in tfidf_array:
        document = [(tfidf[0], (tfidf[1]-minNum)/(maxNum-minNum)) for tfidf in document]          

    return tfidf_array




def get_tfidf(isTrain):
    sentences=[]
    texts=fetch_douban_event(isTrain)
    docs=[]
    #先将词编号成字典
    tok2idx = {}
    for text in texts:
        tokens=sentence2words(text[1].strip(), True)
        docs.append(tokens)
        for idx, tok in enumerate(tokens):
            if tok not in tok2idx:
                tok2idx[tok] = len(tok2idx)
    #生成词袋模型
    corpus=[]
    
    NG = {'>>dummy<<': 0}
    for doc in docs:
        feat={}
        for tok in doc:
            if tok2idx[tok] not in feat:
                feat[tok2idx[tok]] = 1
            feat[tok2idx[tok]] += 1
        corpus.append(feat)


    tfidf = models.TfidfModel(corpus=corpus)

    # 获取模型对所有文本的tfidf值
    tfidf_array=[]
    for document in corpus:
        tfidf_array.append(tfidf[document])


    tfidf_array=normalization(tfidf_array)

    print tfidf_array[0]
    print tfidf[corpus[0]]

    return tfidf_array

if __name__ == '__main__':
    reload(sys)
    sys.setdefaultencoding('utf-8')
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)
 
    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info("running %s" % ' '.join(sys.argv))


    


    w2v_model,class_map=preperation('train_svm_file.svm')
    parms = ['-D 0 -T 0 -I 0 -N 0', '-D 0 -T 0 -I 0 -N 1','-D 0 -T 1 -I 0 -N 0', '-D 0 -T 1 -I 0 -N 1']
    index=0
    for param in parms:
        gt_model=train_model('train_svm_file.svm',param,index)
        ture_ys,predicted_ys=test(w2v_model,gt_model,class_map)
        testResult=GroceryTestResult(ture_ys,predicted_ys)
        print "不分类别的总体准确率："
        print testResult.accuracy_overall
        print "区分类别的准确率:"
        print testResult.accuracy_labels
        print "区分类别的召回率："
        print testResult.recall_labels
        testResult.show_result()
        index+=1


 